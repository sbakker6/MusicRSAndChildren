{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from properties import LYRICS_ROOT_FOLDER\n",
    "from pathlib import Path\n",
    "from fuzzywuzzy import fuzz\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm # Progressbar\n",
    "from utils import create_connection, execute_script, find_first, make_folder_if_not_exists\n",
    "from models.Song import Song\n",
    "from models.SongStructure import SongStructure\n",
    "from models.Section import Section\n",
    "import logging\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "SONG_PROCESSING_FOLDER = make_folder_if_not_exists(LYRICS_ROOT_FOLDER / \"extraction/new2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617afee7",
   "metadata": {},
   "source": [
    "# 1. Parse lyrics.jl with help of artist_info.json\n",
    "\n",
    "In the lyrics.jl file, each line looks like this \n",
    "\n",
    "```json\n",
    "{\"song\": \"Kendrick-lamar-swimming-pools-drank-lyrics\", \"lyrics\": \" ... \"}\n",
    "```\n",
    "\n",
    "We want to parse the song name as provided in this file into (1) artist and (2) song title.\n",
    "\n",
    "In artist_info.json, we have \n",
    "```json\n",
    "{\n",
    "  \"url_name\": \"Kendrick-lamar\",\n",
    "  \"followers\": 23782,\n",
    "  \"roles\": [\n",
    "    \"Verified Artist\",\n",
    "    \"Contributor\"\n",
    "  ],\n",
    "  \"iq\": 39144,\n",
    "  \"songs\": [\n",
    "    \"Kendrick-lamar-humble-lyrics\",\n",
    "    \"A-ap-rocky-fuckin-problems-lyrics\",\n",
    "    \"Kendrick-lamar-maad-city-lyrics\",\n",
    "    \"Kendrick-lamar-swimming-pools-drank-lyrics\",\n",
    "    \"...\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "So ideally, we want to recognize that the artist of the song is \"Kendrick Lamar\", and the song title is \"Humble\". And we can do so by crossreferencing lyrics.jl with artist_info.json.\n",
    "\n",
    "Another strategy is that, of the songs of which this mapping fails, we extract the longest common prefix from the song url (the \"song\" attribute of lyrics.jl) and we can then say with some confidence that this will be the artist name, since we know that the url starts with the artist name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb422077",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_names : list[str] = [] # listing all the distinct artist names from artist_info \n",
    "with open(Path(LYRICS_ROOT_FOLDER, \"artist_info.json\"), 'r') as artist_info:\n",
    "    while (line := artist_info.readline()):\n",
    "        data = json.loads(line)\n",
    "        artist = data['url_name']\n",
    "\n",
    "        if artist in artist_names:\n",
    "            continue\n",
    "    \n",
    "        artist_names.append(artist)\n",
    "\n",
    "\n",
    "# buckets of artist name per first char (lowercase)\n",
    "# buckets are sorted lengthwise (longest first) to make greedy artist name matching faster\n",
    "first_chars = set(map(lambda name: name[0].lower(), artist_names))\n",
    "processed_artist_names = {first_char : sorted([artist_name for artist_name in artist_names if artist_name.lower().startswith(first_char)], key=len, reverse=True) for first_char in first_chars}\n",
    "\n",
    "# Now we crossreference lyrics.jl lines with these artists to the best of our ability\n",
    "songs : list[Song] = []\n",
    "not_found_artists : list[str] = []\n",
    "disregarded_songs : list[str] = []\n",
    "\n",
    "Song.ID = 0 # Reset so that we can run this cell multiple times and starting with id 0\n",
    "\n",
    "with open(Path(LYRICS_ROOT_FOLDER, \"lyrics.jl\"), 'r') as lyrics:\n",
    "    while (line := lyrics.readline()):\n",
    "        data = json.loads(line)\n",
    "        _song_info = data['song']\n",
    "\n",
    "        # There are also \"annotations\" in the lyrics.jl file but we're not interested in them.\n",
    "        if not _song_info.endswith('lyrics'): \n",
    "            disregarded_songs.append(_song_info)\n",
    "            continue\n",
    "\n",
    "        # infer the artist name by taking a look whether there is an exact match with one of the artists in the appropriate bucket\n",
    "        artist_bucket_to_search = processed_artist_names[_song_info[0].lower()]\n",
    "        matched_artist = find_first(lambda artist_name: _song_info.startswith(artist_name), artist_bucket_to_search)\n",
    "\n",
    "        if matched_artist is None:\n",
    "            not_found_artists.append(_song_info)\n",
    "            continue\n",
    "\n",
    "        # extract song title from the song info (given that we now know the artist name):\n",
    "        # example: \" \".join('artist-name-this-is-the-song-title-lyrics'.split('artist-name')[1].split('-')[1:-1]).strip() -> \"this is the song title\"\n",
    "        title = \" \".join(_song_info.split(matched_artist)[1].split('-')[1:-1]).strip()\n",
    "        songs.append(Song(_song_info, title, matched_artist.replace('-', \" \").strip(), data['lyrics']))\n",
    "\n",
    "print(f\"Infered song data for {len(songs)} songs\")\n",
    "if len(not_found_artists) > 0:\n",
    "    print(f\"Failed for {len(not_found_artists)} due to artist mapping not found\")\n",
    "assert len(list(filter(lambda info: not info.endswith('annotated'), disregarded_songs))) == 0, \"All disregarded songs should be annotations\"\n",
    "if len(disregarded_songs) > 0:\n",
    "    print(f\"Disregarded {len(disregarded_songs)} entries (annotations)\")\n",
    "# Strategy to infer new arists names: extract the longest common prefix of the  song (that will be the artist probably)\n",
    "\n",
    "# 1. bucket per first char again to make the process easier (again, also sorted by length -- largest first)\n",
    "not_found_first_chars = set(map(lambda name: name[0].lower(), not_found_artists))\n",
    "not_found_artists_bucketed = {first_char : sorted([artist_name for artist_name in not_found_artists if artist_name.lower().startswith(first_char)], key=len, reverse=True) for first_char in not_found_first_chars}\n",
    "artist_still_not_found : list[str] = []\n",
    "\n",
    "matches = {}\n",
    "extracted_artists = set()\n",
    "\n",
    "for info in not_found_artists:\n",
    "\n",
    "    # Maybe we have already processed the artist\n",
    "    match = find_first(lambda x: info.replace('-', \" \").strip().startswith(x), sorted(extracted_artists, key=len, reverse=True))\n",
    "    if match is not None:\n",
    "        matches[info] = match\n",
    "        continue\n",
    "    \n",
    "    bucket_longest_first = sorted(not_found_artists_bucketed[info.lower()[0]], key=len, reverse=True)\n",
    "    \n",
    "    bucket_size = len(bucket_longest_first)\n",
    "    # We cannot leverage other songs to guess the artist, because this is the only one in there\n",
    "    if bucket_size == 1:\n",
    "        artist_still_not_found.append(info)\n",
    "        continue\n",
    "\n",
    "    # Check matches with others and assign a confidence score (percentage)\n",
    "    others = bucket_longest_first\n",
    "    others.remove(info)\n",
    "    others_ranked = {other : 0 for other in others}\n",
    "    for other in others:\n",
    "        word_pairs = zip(info.split('-'), other.split('-'))\n",
    "        for info_word, other_word in word_pairs:\n",
    "            if info_word != other_word:\n",
    "                break\n",
    "            others_ranked[other] += 1\n",
    "            \n",
    "    ## Now we sort by ranked and see how much they have in common\n",
    "    largest_common_prefix_word_count = max(others_ranked.values())\n",
    "    others_matching = list(filter(lambda x: others_ranked[x] == largest_common_prefix_word_count, others))\n",
    "    # Now we assign the artist to these songs\n",
    "    resolved_artist_name = \" \".join(info.split('-')[:largest_common_prefix_word_count]).strip()\n",
    "    matches[info] = resolved_artist_name\n",
    "# Show the found artists\n",
    "reversed_matches = {}\n",
    "for song, artist in matches.items():\n",
    "    if artist not in reversed_matches.keys():\n",
    "        reversed_matches[artist] = []\n",
    "    reversed_matches[artist].append(song)\n",
    "\n",
    "for artist in sorted(reversed_matches.keys()):\n",
    "    print(artist)\n",
    "    for song in reversed_matches[artist]:\n",
    "        print('\\t', song)\n",
    "\n",
    "new_songs : list[Song] = []\n",
    "# Update the data with the custom found\n",
    "with open(Path(LYRICS_ROOT_FOLDER, \"lyrics.jl\"), 'r') as lyrics:\n",
    "    while (line := lyrics.readline()):\n",
    "        data = json.loads(line)\n",
    "        song_info = data['song']\n",
    "        if song_info not in matches.keys() or matches[song_info] == '':\n",
    "            continue\n",
    "        artist = matches[song_info]\n",
    "        title = \" \".join(song_info.split(artist.replace(\" \", \"-\"))[1].split('-')[1:-1]).strip()\n",
    "        new_songs.append(Song(song_info, title, artist.replace('-', \" \").strip(), data['lyrics']))\n",
    "\n",
    "print(f\"Added {len(new_songs)} new songs\")\n",
    "all_songs = songs + new_songs\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "# MANUAL CORRECTION\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "print(\"STARTING MANUAL CORRECTION\")\n",
    "\n",
    "# Manually correcting song\n",
    "filter_url_name = lambda name : (lambda s : s.genius_url == name)\n",
    "bruno_mars_song1 = find_first(filter_url_name(\"Bruno-mars-thats-what-i-like-gucci-mane-remix-lyrics\"), new_songs)\n",
    "bruno_mars_song2 = find_first(filter_url_name(\"Bruno-mars-thats-what-i-like-partynextdoor-remix-lyrics\"), new_songs)\n",
    "lil_krystalll_song = find_first(filter_url_name(\"Lil-krystalll-tic-tac-demo-lyrics\"), new_songs)\n",
    "lil_silva_song = find_first(filter_url_name(\"Lil-silva-right-for-you-lyrics\"), new_songs)\n",
    "\n",
    "bruno_mars_song1.artist = \"Bruno mars\"\n",
    "bruno_mars_song1.title = \"thats what i like gucci many remix\"\n",
    "\n",
    "bruno_mars_song2.artist = \"Bruno mars\"\n",
    "bruno_mars_song2.title = \"thats what i like partynextdoor remix\"\n",
    "\n",
    "lil_krystalll_song.artist = \"Lil krystalll\"\n",
    "lil_krystalll_song.title = \"tic tac demo\"\n",
    "\n",
    "lil_silva_song.artist = \"Lil silva\"\n",
    "lil_silva_song.title = \"right for you\"\n",
    "\n",
    "\n",
    "assert not any(s.artist == \"Bruno mars thats what i like\" for s in all_songs)\n",
    "\n",
    "print(f\"Fixed {4} songs that were not mapped entirely correctly manually\")\n",
    "\n",
    "\n",
    "# Manually correcting artist names here \n",
    "artist_corrections = {\n",
    "    'A ap rocky': 'A$ap rocky',\n",
    "    'A ap mob': 'A$ap mob',\n",
    "    'A ap ferg': 'A$ap ferg',\n",
    "    'A ap twelvyy': 'A$ap twelvyy',\n",
    "    'A ap nast': 'A$ap nast'\n",
    "}\n",
    "\n",
    "amt_artist_name_corrections = 0\n",
    "for song in all_songs:\n",
    "    if song.artist in artist_corrections:\n",
    "        song.artist = artist_corrections[song.artist]\n",
    "        amt_artist_name_corrections += 1\n",
    "\n",
    "print(f\"Fixed {amt_artist_name_corrections} artist names manually\")\n",
    "\n",
    "manual_url_maps = {\n",
    "    \"Caye-easy-lyrics\": (\"Caye\", \"easy\"),\n",
    "    \"Trav-fuck-and-smoke-lyrics\": (\"Trav\", \"fuck and smoke\"),\n",
    "    \"Amil-4-da-fam-lyrics\": (\"Amil\", \"4 da fam\"),\n",
    "    \"B-smyth-leggo-lyrics\": (\"B-smyth\", \"leggo\"),\n",
    "    \"Chevralet-ss-bad-mother-fucker-bmf-lyrics\": (\"Chevralet-ss\", \"bad mother fucker bmf\"),\n",
    "    \"Cb4-straight-outta-locash-lyrics\": (\"Cb4\", \"straight outta locash\"),\n",
    "    \"Terra-g-clock-work-lyrics\": (\"Terra-g\", \"clock work\"),\n",
    "    \"Quincy-exotic-lyrics\": (\"Quincy\", \"exotic\"),\n",
    "    \"Quin-over-again-lyrics\": (\"Quin\", \"over again\"),\n",
    "    \"Koryn-hawthorne-bright-fire-lyrics\": (\"Koryn-hawthorne\", \"bright fire\"),\n",
    "    \"Acid-drop-king-im-a-problem-lyrics\": (\"Acid-drop-king\", \"im a problem\"),\n",
    "    \"Kda-just-say-lyrics\": (\"Kda\", \"just say\"),\n",
    "    \"Trey-smith-find-you-somewhere-lyrics\": (\"Trey-smith\", \"find you somewhere\"),\n",
    "    \"Bwa-ron-all-i-ever-wanted-lyrics\": (\"Bwa-ron\", \"all i ever wanted\"),\n",
    "    \"Ambre-preach-lyrics\": (\"Ambre\", \"preach\"),\n",
    "    \"Artistes-divers-vald-sofiane-kalash-criminel-biffty-et-suikon-blaze-ad-en-live-dans-planete-rap-lyrics\":\n",
    "        (\"Artistes-divers-vald-sofiane-kalash-criminel-biffty-et-suikon-blaze-ad\", \"en live dans planete rap\"),\n",
    "    \"Catch-lungs-gotta-kill-this-lyrics\": (\"Catch-lungs\", \"gotta kill this\"),\n",
    "    \"Fedy-only-thing-i-know-lyrics\": (\"Fedy\", \"only thing i know\"),\n",
    "    \"Batgang-dangerous-lyrics\": (\"Batgang\", \"dangerous\"),\n",
    "    \"Tamia-officially-missing-you-midi-mafia-mix-with-rap-lyrics\": (\"Tamia\", \"officially missing you midi mafia mix with rap\"),\n",
    "    \"Kold-i-love-the-holy-grail-lyrics\": (\"Kold\", \"i love the holy grail\"),\n",
    "    \"Tamar-braxton-the-one-lyrics\": (\"Tamar-braxton\", \"the one\"),\n",
    "    \"Tracey-lee-keep-your-hands-high-lyrics\": (\"Tracey-lee\", \"keep your hands high\"),\n",
    "    \"Cashflow-harlem-want-my-love-back-lyrics\": (\"Cashflow-harlem\", \"want my love back\"),\n",
    "    \"A1billionaire-rookie-of-the-year-lyrics\": (\"A1billionaire\", \"rookie of the year\"),\n",
    "    \"Louis-mattrs-oops-x-wus-good-lyrics\": (\"Louis-mattrs\", \"oops x wus good\"),\n",
    "    \"T-la-rock-and-jazzy-jay-its-yours-lyrics\": (\"T-la-rock-and-jazzy-jay\", \"its yours\"),\n",
    "    \"Bando-jonez-sex-you-remix-lyrics\": (\"Bando-jonez\", \"sex you remix\"),\n",
    "    \"1wayfrank-make-it-happen-lyrics\": (\"1wayfrank\", \"make it happen\"),\n",
    "    \"Teamcakee-post-to-be-cake-mix-lyrics\": (\"Teamcakee\", \"post to be cake mix\"),\n",
    "    \"Amy-schumer-milk-milk-lemonade-lyrics\": (\"Amy-schumer\", \"milk milk lemonade\"),\n",
    "    \"L-devine-peer-pressure-lyrics\": (\"L-devine\", \"peer pressure\"),\n",
    "    \"Ar-side-god-lyrics\": (\"Ar\", \"side god\"),\n",
    "    \"Vell-oakland-lyrics\": (\"Vell\", \"oakland\"),\n",
    "    \"Cdot-honcho-02-shit-remix-lyrics\": (\"Cdot-honcho\", \"02 shit remix\"),\n",
    "    \"Fekky-way-too-much-lyrics\": (\"Fekky\", \"way too much\"),\n",
    "    \"Ar15-cant-see-me-again-lyrics\": (\"Ar15\", \"cant see me again\"),\n",
    "    \"Yaprak-asimov-save-me-lyrics\" : (\"Yaprak-asimov\", \"save-me\"),\n",
    "    \"Usda-white-girl-remix-lyrics\": (\"Usda\", \"white-girl-remix\")\n",
    "}\n",
    "\n",
    "# Manually correcting the stuff \n",
    "manual_songs = []\n",
    "with open(Path(LYRICS_ROOT_FOLDER, \"lyrics.jl\"), 'r') as lyrics:\n",
    "    while (line := lyrics.readline()):\n",
    "        data = json.loads(line)\n",
    "        genius_url = data['song']\n",
    "        if genius_url not in manual_url_maps.keys():\n",
    "            continue\n",
    "\n",
    "        artist, title = manual_url_maps[genius_url]\n",
    "        manual_songs.append(Song(genius_url, title, artist.replace('-', \" \").strip(), data['lyrics']))\n",
    "\n",
    "all_songs += manual_songs\n",
    "print(f\"Added {len(manual_songs)} songs manually\")\n",
    "print(f\"In total, we have {len(all_songs)} songs\")\n",
    "\n",
    "\n",
    "# Check that no songs are left unmapped\n",
    "all_mapped_urls = set(map(lambda s: s.genius_url, all_songs))\n",
    "unmapped=0\n",
    "with open(Path(LYRICS_ROOT_FOLDER, \"lyrics.jl\"), 'r') as lyrics:\n",
    "    while (line := lyrics.readline()):\n",
    "        data = json.loads(line)\n",
    "        genius_url = data['song']\n",
    "        if not genius_url.endswith(\"annotated\") and genius_url not in all_mapped_urls:\n",
    "            print(\"Unmapped URL:\", genius_url)\n",
    "            unmapped += 1\n",
    "    \n",
    "    print(f\"{unmapped} unmapped urls\")\n",
    "assert unmapped == 0, f\"There are still {unmapped} urls left unprocessed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the songs to a csv file:\n",
    "with open(Path(SONG_PROCESSING_FOLDER, \"lyrics_tracks.tsv\"), 'w') as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerow([\"track_id\", \"artist_name\", \"track_name\", \"genius_url\"])\n",
    "    for song in all_songs:\n",
    "        writer.writerow([song.id, song.artist, song.title, song.genius_url])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35305aff",
   "metadata": {},
   "source": [
    "# 2. Crossreference the data: Fuzzy matching\n",
    "## How to link the LFM2B dataset to this extracted data?\n",
    "\n",
    "We have extracted song artists and titles for which we have the lyrics available from lyrics.jl.\n",
    "These are now neatly written to a tsv file where each song also has an ID. \n",
    "\n",
    "So, the idea is to create a new file in which we link tracks by their ID in the LFM2B dataset and their ID in the genius lyrics dataset. \n",
    "\n",
    "The matching process is inherently inprecise, as tracks might not all be represented in the same way in both datasets. \n",
    "Therefore, we do \"fuzzy matching\" per song on both the artist as well as the track name. \n",
    "\n",
    "So each song from the genius lyrics database (GL) matches to zero or more artists in LFM2B and to zero or more track names in LFM2B. \n",
    "\n",
    "Then we will assign a confidence score for the artist and for the track name using a fuzzy matching library `fuzzywuzzy`. This yields a score \\[0-100\\] (percentage overlap; capitalization ignored).\n",
    "\n",
    "We simply rank the matches on their score (highest score = rank 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only consider the tracks where we could have lyrics (non-instrumental)\n",
    "\n",
    "con, cursor = create_connection(\"lfm2b\")\n",
    "cursor.execute(r\"\"\"\n",
    "               create materialized view if not exists track_non_instrumental as select * from track natural join artist where track_name not ilike '%instrumental%';\n",
    "               create index if not exists tni_track_name_idx on track_non_instrumental(track_name)\n",
    "               \"\"\")\n",
    "con.commit()\n",
    "print(\"Created track_non_instrumental\")\n",
    "\n",
    "\n",
    "class LFM2BMatch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        song,\n",
    "        lfm2b_artist_id,\n",
    "        lfm2b_artist_name,\n",
    "        artist_confidence,\n",
    "        lfm2b_track_id,\n",
    "        lfm2b_track_name,\n",
    "        track_confidence,\n",
    "    ):\n",
    "        self.song = song\n",
    "        self.lfm2b_artist_id = lfm2b_artist_id\n",
    "        self.lfm2b_artist_name = lfm2b_artist_name\n",
    "        self.lfm2b_track_id = lfm2b_track_id\n",
    "        self.lfm2b_track_name = lfm2b_track_name\n",
    "        self.artist_confidence = artist_confidence\n",
    "        self.track_confidence = track_confidence\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.lfm2b_artist_name} - {self.lfm2b_track_name}\"\n",
    "\n",
    "\n",
    "matches: list[LFM2BMatch] = []\n",
    "non_matched: list[Song] = []\n",
    "errors: list[tuple[Song, Exception]] = []\n",
    "\n",
    "\n",
    "def setup_logger(root_folder=SONG_PROCESSING_FOLDER, logger_name='main_logger') -> logging.Logger :\n",
    "    \"\"\"\n",
    "    Creates a logger with the given name that logs to `lfm2b-genius-mapping_{timestamp}.log` in the specified folder\n",
    "\n",
    "    Args:\n",
    "        root_folder : folder where the logfile will be placed. Must be an existing folder. The default is the global `SONG_PROCESSING_FOLDER`\n",
    "        logger_name : name of the logger. The default is `main_logger`. Useful to distinguish loggers when multithreading. \n",
    "    \n",
    "    Returns:\n",
    "        logger : The created logger\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%dT%H_%M_%S\")\n",
    "    MAPPING_LOG_PATH = Path( root_folder, f\"lfm2b-genius-mapping_{timestamp}.log\")\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False \n",
    "\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    file_handler = logging.FileHandler(MAPPING_LOG_PATH, encoding='utf-8')\n",
    "    formatter = logging.Formatter(\"%(levelname)s - %(asctime)s: %(message)s\", datefmt=\"%d-%m-%Y,%H:%M:%S\")\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def similarity_score(a: str, b: str) -> int:\n",
    "    \"\"\"Return a score [0-100] on how similar `a` is to `b` using the Levenshtein distance. Casing is ignored.\"\"\"\n",
    "    return fuzz.ratio(a.lower(), b.lower())\n",
    "\n",
    "\n",
    "def find_best_track_match(\n",
    "    song: Song, artists: list, tracks: list, write_function, logger\n",
    ") -> LFM2BMatch | None:\n",
    "    \"\"\"\n",
    "    Extracts the best match of GL song to LFM2b track according to the procedure descibred in the above cell.\n",
    "    Given a song (GLDB), matched artists (LFM2B) and matched tracks (LFM2B), ranks the matches and returns the best ranked match if possible.\n",
    "    If no tracks are matched in the first place, then no match can be made, so `None` is returned.\n",
    "\n",
    "    Matches are directly written with the `write_function` and the process is logged to file.\n",
    "\n",
    "    Args:\n",
    "        song : GL song to find a matching LFM2b track to\n",
    "        artists : List of likely LFM2b artist matches in the format [(artist_id, artist_name), ...]\n",
    "        tracks : List of likely LFM2b track matches in the format [(track_id, track_name, artist_id, artist_name), ...]\n",
    "        write_function : Function that takes in a `LFM2BMatch` object or `None` and writes it to file (or handles it in any other way deemed fit)\n",
    "        logger : Logger to log the matching process with\n",
    "    \n",
    "    Returns:\n",
    "        match : `LFM2BMatch` object with the best GL <-> LFM2b match, or `None` if no match could be made. \n",
    "    \"\"\"\n",
    "\n",
    "    # Create the ranking\n",
    "\n",
    "    artist_name_ranking = []\n",
    "    for artist_id, artist_name in artists:\n",
    "        score = similarity_score(song.artist, artist_name)\n",
    "        artist_name_ranking.append(\n",
    "            (\n",
    "                song.artist,\n",
    "                artist_id,\n",
    "                artist_name,\n",
    "                score\n",
    "            )\n",
    "        )\n",
    "\n",
    "    track_name_ranking = []\n",
    "    for track_id, track_name, artist_id, artist_name in tracks:\n",
    "        track_name_ranking.append(\n",
    "            (\n",
    "                song.title,\n",
    "                track_id,\n",
    "                track_name,\n",
    "                artist_id,\n",
    "                artist_name,\n",
    "                similarity_score(song.title, track_name),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    artist_name_ranking.sort(key=lambda x: x[3], reverse=True)\n",
    "    track_name_ranking.sort(key=lambda x: x[5], reverse=True)\n",
    "\n",
    "    LIMIT = 10  # limit the amount of logged rankings for file size purposes\n",
    "    logger.info(\"\\t artist ranking\")\n",
    "    for artist_ranking in artist_name_ranking[:LIMIT]:\n",
    "        logger.info(\"\\t\\t %s\", artist_ranking)\n",
    "\n",
    "    logger.info(\"\\t track ranking\")\n",
    "    for track_ranking in track_name_ranking[:LIMIT]:\n",
    "        logger.info(\"\\t\\t %s\", track_ranking)\n",
    "\n",
    "    logger.info(\"\\t verdict\")\n",
    "\n",
    "    best_match: LFM2BMatch | None = None  # We assume no match (for example if there are no eligible tracks)\n",
    "\n",
    "    # Ideally, we have the best ranked track (name) with the highest best ranked artist name\n",
    "    found_match = False\n",
    "    if len(track_name_ranking) > 0 and len(artist_name_ranking) > 0:\n",
    "        for ranked_track in track_name_ranking:\n",
    "            if found_match:\n",
    "                break\n",
    "            for ranked_artist in artist_name_ranking:\n",
    "                if ranked_track[3] == ranked_artist[1]: # If the artist of the track is the best ranked matched artist, then we pre-emptively say we have a match.\n",
    "                    best_match = LFM2BMatch(\n",
    "                        song,\n",
    "                        lfm2b_artist_id=ranked_artist[1],\n",
    "                        lfm2b_artist_name=ranked_artist[2],\n",
    "                        artist_confidence=ranked_artist[-1],\n",
    "                        lfm2b_track_id=ranked_track[1],\n",
    "                        lfm2b_track_name=ranked_track[2],\n",
    "                        track_confidence=ranked_track[-1],\n",
    "                    )\n",
    "                    found_match = True\n",
    "                    break\n",
    "    elif len(track_name_ranking) > 0:\n",
    "        best_ranked_track = track_ranking[0]\n",
    "        # We have only track information, so just get the best track\n",
    "        best_match = LFM2BMatch(\n",
    "            song,\n",
    "            lfm2b_artist_id=best_ranked_track[3],\n",
    "            lfm2b_artist_name=best_ranked_track[4],\n",
    "            artist_confidence=0,\n",
    "            lfm2b_track_id=best_ranked_track[1],\n",
    "            lfm2b_track_name=best_ranked_track[2],\n",
    "            track_confidence=best_ranked_track[-1],\n",
    "        )\n",
    "\n",
    "    logger.info(\"\\t\\t %s\", best_match)\n",
    "    write_function(\n",
    "        best_match\n",
    "    )  # write the match to file, note that an exception will be raised when there is no match found (None). This is caught in the error logfile on purpose.\n",
    "\n",
    "\n",
    "def do_song_mapping(songs, cursor, logger, results_folder=SONG_PROCESSING_FOLDER, artist_cache={}):\n",
    "    \"\"\"\n",
    "    Encapsulates the entire process of mapping GL songs to LFM2b tracks.\n",
    "\n",
    "    Args:\n",
    "        songs: List of `Song` objects representing the GL songs that should be matched to LFM2b tracks.\n",
    "        cursor: Cursor to the 'lfm2b' postgres database\n",
    "        logger: Logger to log the process with\n",
    "        results_folder: Folder to which the mapping process will be written (log, errors and results). Must be a pre-existing folder. Defaults to the global `SONG_PROCESSING_FOLDER`\n",
    "        artist_cache: Dictionary serving as cache in the matching process, reducing the need for expensive database queries. \n",
    "\n",
    "    \"\"\"\n",
    "    global last_song_id\n",
    "    start = songs[0].id\n",
    "    end = songs[-1].id\n",
    "    with open(results_folder / f\"errors_{start}-{end}.tsv\", \"w\") as error_file:\n",
    "        error_file.write(\"track_id\\ttrack\\terror\\n\")\n",
    "        mapping_path = Path(results_folder, f\"lfm2b_genius_track_mapping_{start}-{end}.tsv\")\n",
    "        assert (not mapping_path.exists()), f\"WARNING. You are about to overwrite this file {mapping_path}\"\n",
    "        with open(mapping_path, \"w\", encoding=\"utf8\") as f:\n",
    "            writer = csv.writer(f, delimiter=\"\\t\")\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    \"lfm2b_track_id\",\n",
    "                    \"genius_lyrics_track_id\",\n",
    "                    \"artist_confidence\",\n",
    "                    \"track_confidence\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            def write_function(match : LFM2BMatch | None):\n",
    "                if match is None:\n",
    "                    raise RuntimeError(\"None type has no attribute lfm2b_track_id\")\n",
    "                writer.writerow([match.lfm2b_track_id, match.song.id, match.artist_confidence, match.track_confidence])\n",
    "\n",
    "            for i in tqdm(range(len(songs))):\n",
    "                song = songs[i]\n",
    "                try:\n",
    "                    last_song_id = song.id\n",
    "                    logger.info(\n",
    "                        \"Computing match for song %d: %s - %s\",\n",
    "                        song.id,\n",
    "                        song.artist,\n",
    "                        song.title,\n",
    "                    )\n",
    "\n",
    "                    if song.title.strip() == \"\" or song.artist.strip() == \"\":\n",
    "                        logger.info(\"\\tSkipping due to empty title or artist\")\n",
    "                        raise RuntimeError(\"Empty song artist or title\")  # To collect this in the error_file, we raise exception\n",
    "                    \n",
    "                    # Shortcut: only lookup artist if we have not already cached them\n",
    "                    if song.artist not in artist_cache.keys():\n",
    "                        cursor.execute(f\"SELECT artist_id, artist_name FROM artist WHERE artist_name ilike '%{song.artist}%'\")\n",
    "                        artists = cursor.fetchall()\n",
    "                        artist_cache[song.artist] = artists\n",
    "                    else:\n",
    "                        logger.info(\"\\tUsing cache for artist\")\n",
    "                        artists = artist_cache[song.artist]\n",
    "\n",
    "                    cursor.execute(f\"SELECT track_id, track_name, artist_id, artist_name FROM track_non_instrumental WHERE track_name ilike '%{song.title}%'\")\n",
    "                    tracks = cursor.fetchall()\n",
    "\n",
    "                    if len(tracks) == 0:\n",
    "                        logger.warning(\"\\t No matches for this track, skipping...\")\n",
    "\n",
    "                    # Discard the result, because the result is also written to file (this process takes 50+ hrs, so we reconstruct the mapping from these files later)\n",
    "                    _ = find_best_track_match(song, artists, tracks, write_function, logger)\n",
    "                except Exception as e:\n",
    "                    error_file.write(f\"{song.id}\\t{song}\\t{e}\\n\")\n",
    "                    errors.append((song, e))\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# SETUP\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "START_SONG_ID = 0 # Inclusive\n",
    "END_SONG_ID = -1  # Inclusive\n",
    "last_song_id = None\n",
    "RESULTS_FOLDER = make_folder_if_not_exists(SONG_PROCESSING_FOLDER/\"single_threaded\")\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "if (END_SONG_ID - START_SONG_ID) < 0:\n",
    "    con.close()\n",
    "    raise RuntimeError(\"No songs selected for mapping\")\n",
    "\n",
    "print(\"Starting...\")\n",
    "\n",
    "logger = setup_logger(RESULTS_FOLDER)\n",
    "songs_to_map = list(filter( lambda s: (s.id >= START_SONG_ID and s.id <= END_SONG_ID), all_songs))\n",
    "do_song_mapping(songs_to_map, cursor, logger, results_folder=RESULTS_FOLDER)\n",
    "logger.handlers.clear() # release the files from the logger\n",
    "con.close()\n",
    "\n",
    "print(f\"Had {len(errors)} errors\")\n",
    "print(f\"Last song id {last_song_id}\")  # Useful for monitoring the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a5341",
   "metadata": {},
   "source": [
    "## Or use multithreading for processing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False, \"Set True if multithreading is wanted\"\n",
    "\n",
    "import threading\n",
    "\n",
    "THREADS = 10\n",
    "\n",
    "all_songs.sort(key=lambda s: s.id)\n",
    "all_song_ids = list(map(lambda s : s.id, all_songs))\n",
    "\n",
    "assert all(a <= b for a, b in zip(all_song_ids, all_song_ids[1:])), \"IDs should be sorted!\"\n",
    "min_song_id, max_song_id = 32400, 37447\n",
    "assert all_songs[-1].id == max_song_id, \"Final id not matching\"\n",
    "\n",
    "print(f\"{datetime.now()} - Starting multithreaded processing of songs {min_song_id} through {max_song_id} on {THREADS} threads.\")\n",
    "\n",
    "chunck_size  = int(np.ceil((max_song_id - min_song_id)/THREADS))\n",
    "\n",
    "FOLDER = make_folder_if_not_exists(SONG_PROCESSING_FOLDER / f\"threaded_{min_song_id}_{max_song_id}\")\n",
    "\n",
    "threads = []\n",
    "connections = []\n",
    "for i in range(THREADS):\n",
    "    start =min_song_id + i * chunck_size\n",
    "    end = start + chunck_size\n",
    "    songs_to_map = list(filter( lambda s: (s.id >= start and s.id <= end), all_songs))\n",
    "    thread_folder = make_folder_if_not_exists(FOLDER / f\"thread_{i+1}\")\n",
    "    \n",
    "    conn, cursor = create_connection(\"lfm2b\")\n",
    "    connections.append((conn, cursor))\n",
    "    \n",
    "    logger = setup_logger(thread_folder, logger_name=f\"logger_thread_{i+1}\")\n",
    "    threads.append(threading.Thread(target=do_song_mapping, args=(songs_to_map, cursor, logger), kwargs={\"results_folder\": thread_folder}))\n",
    "    \n",
    "for t in threads:\n",
    "    t.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "for conn, cursor in connections:\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e93d5",
   "metadata": {},
   "source": [
    "## Fixing failed mappings\n",
    "\n",
    "To see which tracks failed, we categorize them:\n",
    "\n",
    "1. Loop through all the \"error\" csv files and group on 'error' column\n",
    "2. Loop trough all the track mappings and see whether the mappings are valid (integer keys for example)\n",
    "\n",
    "We collect those songs for which the mapping failed and we go through them again if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files_in_folder_matching_to(folder : str, pattern: str):\n",
    "    \"\"\"Given a folder path, lists all the files in that folder that match the given regex pattern.\"\"\"\n",
    "    return list(filter(lambda fn : bool(re.match(pattern, fn)), os.listdir(folder))) \n",
    "\n",
    "\n",
    "error_files = find_files_in_folder_matching_to(SONG_PROCESSING_FOLDER, r\"errors_\\d+-\\d+.tsv\") \n",
    "error_files.sort(key=lambda fname: int(fname[7:-4].split('-')[0])) # Sort on the start ID of chunks, such that we process errors_10-20.tsv before errors_20-30.tsv for example\n",
    "\n",
    "errors = {}\n",
    "for error_file in error_files:\n",
    "    with open(SONG_PROCESSING_FOLDER / error_file, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        next(reader) # skip headers\n",
    "        for line in reader:\n",
    "            if line == []:\n",
    "                continue\n",
    "            track_id, _, err = line\n",
    "            if err not in errors.keys():\n",
    "                errors[err] = []\n",
    "            errors[err].append(track_id)\n",
    "\n",
    "for err, song_ids in errors.items():\n",
    "    print(f\"{err}: {len(song_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56268ec0",
   "metadata": {},
   "source": [
    "# 3. Create the partial Genius Lyrics (GL) Postgres database\n",
    "\n",
    "We now have all data needed to write to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, we reconstruct the GL song_id <-> LFM2b track_id mapping from the written files by the painstakingly long process from earlier. \n",
    "# We keep deal with duplicate mappings as well\n",
    "\n",
    "class DuplicateException(RuntimeError):\n",
    "    \"\"\" Class to signify duplicate mappings \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "\n",
    "lfm2b_to_genius = dict()\n",
    "# These dictionaries are these for monitoring purposes\n",
    "map_errs : dict[RuntimeError, list[int]]= dict() # For tracking failed mappings that ended up in the tsv files. Grouped per error type\n",
    "id_seen_in_files : dict[int, list[str]] = dict() # For tracking duplicate GENIUS ids (they may end up in multiple files due to the fragmented mapping process of start/end ID)\n",
    "\n",
    "mapping_files = find_files_in_folder_matching_to(SONG_PROCESSING_FOLDER, r\"lfm2b_genius_track_mapping_\\d+-\\d+.tsv\")\n",
    "for track_mapping in mapping_files: \n",
    "    with open(SONG_PROCESSING_FOLDER / track_mapping) as lyrics_tracks_mapping:\n",
    "        reader = csv.reader(lyrics_tracks_mapping, delimiter='\\t')\n",
    "        next(reader) # skip headers\n",
    "        for line in reader:\n",
    "            if line == []: # Might be empty lines in between records due to the way they were logged\n",
    "                continue\n",
    "            try:\n",
    "                lfm2b_track_id, genius_track_id, artist_confidence, track_confidence = line\n",
    "                \n",
    "                lfm2b_track_id_int = int(lfm2b_track_id)\n",
    "                genius_track_id_int = int(genius_track_id)\n",
    "                artist_confidence_int = int(artist_confidence)\n",
    "                track_confidence_int = int(track_confidence)\n",
    "\n",
    "                if genius_track_id not in id_seen_in_files.keys():\n",
    "                    id_seen_in_files[genius_track_id] = []\n",
    "\n",
    "                id_seen_in_files[genius_track_id].append(lyrics_tracks_mapping.name)\n",
    "\n",
    "                # It can happen that multiple Genius songs map to the same LFM2b song according to the fuzzy matching...\n",
    "                # In that case, we need to investigate which song fits better...\n",
    "                if lfm2b_track_id_int in lfm2b_to_genius.keys():\n",
    "                    (old_mapping, old_ac, old_tc, old_file) = lfm2b_to_genius[lfm2b_track_id_int]\n",
    "                    print(f\"WARNING! Duplicate mapping found to LFM2B track {lfm2b_track_id_int}:\\n\\tgenius track {old_mapping} ({old_ac}, {old_tc}) from {old_file}\\n\\tgenius track {genius_track_id_int} ({artist_confidence}, {track_confidence}) from {track_mapping}\")\n",
    "                    old_score = old_ac + old_tc\n",
    "                    new_score = artist_confidence_int + track_confidence_int\n",
    "                    if old_mapping == genius_track_id or old_score > new_score:\n",
    "                        # No overwrite when we have a duplicate mapping\n",
    "                        continue\n",
    "\n",
    "                    # We are overwriting the old mapping, so we better record this explicitly to handle it later\n",
    "                    print(f\"\\toverwriting due to better total score... ({new_score} vs {old_score})\")\n",
    "                    if DuplicateException not in map_errs.keys():\n",
    "                        map_errs[DuplicateException] = []\n",
    "                    map_errs[DuplicateException].append(old_mapping)\n",
    "                    \n",
    "\n",
    "                lfm2b_to_genius[lfm2b_track_id_int] = (genius_track_id_int, artist_confidence_int, track_confidence_int, track_mapping)\n",
    "                \n",
    "\n",
    "            except Exception as e:\n",
    "                tp = type(e)\n",
    "                if tp not in map_errs.keys():\n",
    "                    map_errs[tp] = []\n",
    "                map_errs[tp].append(int(genius_track_id))\n",
    "\n",
    "print(f\"{len(lfm2b_to_genius.keys())} successful mapping\")\n",
    "print(f\"{len(map_errs.keys())} error types: {map_errs.keys()}\")\n",
    "for id, files in id_seen_in_files.items():\n",
    "    if len(files) > 1:\n",
    "        print(f\"Saw {id} in files {files}\")\n",
    "\n",
    "def song_id_in_filter(id_list):\n",
    "    \"\"\"Returns a filter that can be aplied to a list of Song objects to filter out songs that have their id in `id_list`\"\"\"\n",
    "    return lambda song : song.id in id_list\n",
    "\n",
    "failed_maps = set(err for failed_per_error in map_errs.values() for err in failed_per_error) # flattens all errors, regardless of err type\n",
    "failed_songs = set(filter(song_id_in_filter(failed_maps), all_songs))\n",
    "\n",
    "assert len(failed_songs) == len(failed_maps), f\"Could not map all failures to songs; failed_maps {len(failed_maps)}, failed_songs {len(failed_songs)}\"\n",
    "\n",
    "# logger = setup_logger()\n",
    "# do_song_mapping(list(failed_songs), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_genius_db(lfm_genius_mappings : dict[int, tuple[int, int, int, str]], songs : list[Song]):\n",
    "    ''' \n",
    "    Creates the following tables (populated or not yet populated) in the genius_db on the Postgres server. \n",
    "        POPULATED\n",
    "        song            - song_id, genius_url, arist_name, song_name\n",
    "        lfm2b_genius    - song_id, genius_url, artist_confidence, track_confidence\n",
    "        lyrics          - song_id, lyrics\n",
    "\n",
    "        NOT YET POPULATED\n",
    "        cluster          - song_id, cluster_id\n",
    "\n",
    "        The genius_db must exist on the server (`CREATE DATABASE genius_db` should already be run)\n",
    "\n",
    "    Args:\n",
    "        lfm_genius_mappings : mapping of LFM2b track_id key to GL (song_id, artist_confidence, track_confidence, song_url)\n",
    "        songs               : list of all songs to put in the database. It is expected that for each mapping, the corresponding song exists in this list. \n",
    "    '''\n",
    "    con, cursor = create_connection('genius_db')\n",
    "    with con: # execute in transaction\n",
    "        execute_script(Path(\"genius_lyrics_schemas\", \"song.sql\"), cursor)\n",
    "        execute_script(Path(\"genius_lyrics_schemas\", \"lfm2b_genius.sql\"), cursor)\n",
    "        execute_script(Path(\"genius_lyrics_schemas\", \"lyrics.sql\"), cursor)\n",
    "        execute_script(Path(\"genius_lyrics_schemas\", \"cluster.sql\"), cursor) # We create it, but we do not populate it yet\n",
    "        for track_id, (song_id, ac, tc, _) in lfm_genius_mappings.items():\n",
    "            song = find_first(lambda s : s.id == song_id, songs)\n",
    "            assert song is not None, f\"Could not find song for this mapping! Track id {track_id}, song id {song_id}\"\n",
    "            assert song.id == song_id, \"Just checking find_first\"\n",
    "            cursor.execute(\"\"\"INSERT INTO song (song_id, genius_url, artist_name, song_name)\n",
    "                           VALUES (%s, %s, %s, %s)\n",
    "                           \"\"\", [song_id, song.genius_url, song.artist, song.title])\n",
    "            cursor.execute(\"\"\"INSERT INTO lfm2b_genius (track_id, song_id, artist_confidence, track_confidence)\n",
    "                           VALUES (%s, %s, %s, %s)\"\"\", [track_id, song_id, ac, tc])    \n",
    "            cursor.execute(\"\"\"INSERT INTO lyrics (song_id, lyrics) \n",
    "                           VALUES (%s, %s)\"\"\", [song_id, song.lyrics])\n",
    "    con.close()\n",
    "\n",
    "create_genius_db(lfm2b_to_genius, all_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61120e8",
   "metadata": {},
   "source": [
    "## What is left of the listening events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcon, gcur = create_connection(\"genius_db\")\n",
    "lfmcon, lfmcur = create_connection(\"lfm2b\")\n",
    "\n",
    "gcur.execute(\"select track_id from lfm2b_genius;\")\n",
    "lfm2b_track_ids = gcur.fetchall()\n",
    "\n",
    "lfmcur.execute(\"select age_at_listen, count(*) from listening_event le where le.track_id in %s group by age_at_listen\", (tuple(lfm2b_track_ids),))\n",
    "per_age = pd.DataFrame(lfmcur.fetchall(), columns=[\"age_at_listen\", \"amt_le\"])\n",
    "\n",
    "gcon.close()\n",
    "lfmcon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575775c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from properties import CSV_FOLDER\n",
    "\n",
    "count_by_age_folder = CSV_FOLDER / \"count-by-age\"\n",
    "\n",
    "\n",
    "per_age.plot.bar(x='age_at_listen')\n",
    "\n",
    "per_age.to_csv(count_by_age_folder/\"listening_events_count-by-age_matched_with_genius.tsv\", sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f42f3",
   "metadata": {},
   "source": [
    "# 3. Extracting structure of the lyrics\n",
    "\n",
    "This can be executed standalone without running the previous sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7909bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load all the genius songs from the newly created database\n",
    "from utils import create_connection\n",
    "conn, cursor = create_connection(\"genius_db\")\n",
    "cursor.execute(\"select song_id, track_id, artist_confidence, track_confidence, genius_url, song_name, artist_name, lyrics from song natural join lyrics natural join lfm2b_genius;\")\n",
    "\n",
    "class GeniusSong(Song):\n",
    "    def __init__(self, song_id, track_id, artist_confidence, track_confidence, genius_url, title, artist, lyrics):\n",
    "        super().__init__(genius_url, title, artist, lyrics)\n",
    "        self.track_confidence = track_confidence\n",
    "        self.artist_confidence = artist_confidence\n",
    "        self.track_id = track_id\n",
    "        self.id = song_id\n",
    "\n",
    "genius_db_songs : list[GeniusSong] = list(map(lambda data : GeniusSong(*data),cursor.fetchall()))\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc36e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_TO_SECTION : dict[str, Section]= {\n",
    "        \"Intro\" : Section.INTRO,\n",
    "        \"Chorus\" : Section.CHORUS,\n",
    "        \"Pre-Chorus\" : Section.PRE_CHORUS,\n",
    "        \"Post-Chorus\" : Section.POST_CHORUS,\n",
    "        \"Refrain\" : Section.CHORUS,\n",
    "        \"Bridge\" : Section.BRIDGE,\n",
    "        \"Verse\" : Section.VERSE,\n",
    "        \"Outro\" : Section.OUTRO,\n",
    "        \"Interlude\" : Section.INSTRUMENTAL,\n",
    "        \"Hook\" : Section.HOOK,\n",
    "        \"Pre-Hook\" : Section.PRE_HOOK,\n",
    "        \"Post-Hook\" : Section.POST_HOOK,\n",
    "        \"Post Hook\" : Section.POST_HOOK,\n",
    "        \"Break\" : Section.BREAK,\n",
    "        \"Couplet\" : Section.VERSE,\n",
    "        \"Piano Solo\" : Section.SOLO,\n",
    "        \"Guitar Solo\" : Section.SOLO,\n",
    "        \"Verso\" : Section.VERSE,\n",
    "        \"Pre-Coro\" : Section.PRE_CHORUS,\n",
    "        \"Pont\" : Section.BRIDGE,\n",
    "        \"Instrumental\" : Section.INSTRUMENTAL,\n",
    "        \"Zwrotka\": Section.VERSE, # Verse in Polish,\n",
    "        \"Refren\": Section.CHORUS, # Refrain in polish,\n",
    "        \"Куплет\": Section.VERSE, # Verse in Russian/Bulgarian...\n",
    "        \"Strofa\": Section.VERSE, #Verse in Italian,\n",
    "        \"Vamp\" : Section.VAMP\n",
    "}\n",
    "\n",
    "# If the brackets satisfy any of these predicates, then they will be marked ignored (\"-\")\n",
    "IGNORES = [\n",
    "    lambda x: x.lower().startswith(\"produced by\")\n",
    "]\n",
    "\n",
    "def map_to_section(text: str) -> Section | None:\n",
    "    \"\"\" Returns the corresponding section of a label. If this label is to be ignored (is not a section) according to the rules in IGNORES, then returns None, otherwise returns the Section\"\"\"\n",
    "    if any([ignore_predicate(text) for ignore_predicate in IGNORES]):\n",
    "        return None\n",
    "\n",
    "    low = text.lower()\n",
    "    contenders = [label for label in LABEL_TO_SECTION.keys() if label.lower() in low]\n",
    "    if not any(contenders):\n",
    "        return Section.OTHER\n",
    "    \n",
    "    longest_matched_label = sorted(contenders, key=len, reverse=True)[0] # longest contender wins\n",
    "    return LABEL_TO_SECTION[longest_matched_label]\n",
    "\n",
    "\n",
    "OPENING_CHAR, CLOSING_CHAR = (\"[\", \"]\")\n",
    "\n",
    "def validate_brackets(lyrics: str):\n",
    "    \"\"\" Makes sure that there are as many opening as closing brackets in the lyrics. Throws a ValueError otherwise.\"\"\"\n",
    "    stack = 0\n",
    "    for char in lyrics:\n",
    "        if char == OPENING_CHAR:\n",
    "            stack += 1\n",
    "\n",
    "        if char == CLOSING_CHAR:\n",
    "            stack -= 1\n",
    "\n",
    "    if stack != 0:\n",
    "        raise ValueError(\"Invalid brackets\")\n",
    "\n",
    "def find_all_brackets(lyrics: str) -> list[str]:\n",
    "    \"\"\" Extracts bracket pairs and the content within those brackets using the two-pointers method. In lyrics the brackets contain the section labels (i.e. '[Intro]')\"\"\"\n",
    "    validate_brackets(lyrics)\n",
    "    brackets = []\n",
    "    i = 0\n",
    "    while i < len(lyrics):\n",
    "        if lyrics[i] != OPENING_CHAR:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        j = i\n",
    "        while lyrics[j] != CLOSING_CHAR:\n",
    "            j += 1\n",
    "        brackets.append(lyrics[i:j+1])\n",
    "        i = j\n",
    "    return brackets\n",
    "\n",
    "def generate_section_descriptor(structure : list[Section], section : Section) -> SongStructure.SectionDescriptor:\n",
    "   \"\"\" Generates descriptors for a section given the song structure (amount pos, average position, std_dev, etc)\"\"\" \n",
    "   amt = sum(1 if sec == section else 0 for sec in structure)\n",
    "   positions = list(filter(lambda x : x is not None, [i if structure[i] == section else None for i in range(len(structure))]))\n",
    "   avg_pos = np.average(positions) if len(positions) > 0 else 0\n",
    "   std_pos = np.std(positions) if len(positions) > 0 else 0 \n",
    "   return SongStructure.SectionDescriptor(amt, avg_pos, std_pos)\n",
    "\n",
    "\n",
    "def structure_extraction_strat1(lyrics: str) -> SongStructure:\n",
    "    '''\n",
    "    Given a string of the entire lyrics of a song, we: \n",
    "\n",
    "        1. Extract the labels from the lyrics\n",
    "        2. Map relevant labels to sections, ignoring irrelevant labels\n",
    "        3. Store the sections in the correct order as song structure list \n",
    "        4. Create a SongStructure instance, descibing the relevant details\n",
    "\n",
    "    Args:\n",
    "        lyrics : full lyrics of a song, including section annotations (as from Genius.com)\n",
    "\n",
    "    Returns:\n",
    "        song_structure : Song structure fingerprint\n",
    "    '''\n",
    "    brackets = find_all_brackets(lyrics)                        # ['[Intro: foo]', '[Verse 1], ...]\n",
    "    brackets_content = list(map(lambda b: b[1:-1], brackets))   # ['Intro: foo'  , 'Verse 1', ...]\n",
    "    parts : list[Section] = list(filter(lambda x: x is not None, map(map_to_section, brackets_content)))\n",
    "    failures : list[str] = []\n",
    "    for i, part in enumerate(parts):\n",
    "        if part is None:\n",
    "            failures.append(brackets_content[i])\n",
    "    section_descriptors = {section : generate_section_descriptor(parts, section) for section in Section}\n",
    "    return SongStructure(parts, failures, section_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c404c13",
   "metadata": {},
   "source": [
    "# 4. Clustering the structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65243896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the song structure\n",
    "errors = 0\n",
    "all_rows = []\n",
    "for song in genius_db_songs:\n",
    "    try:\n",
    "        song_structure : SongStructure = song.extract_structure(structure_extraction_strat1)\n",
    "        \n",
    "        values = [song.id]\n",
    "        for section in Section:\n",
    "            values.extend([song_structure.descriptors[section].amount,\n",
    "                        song_structure.descriptors[section].pos_avg,\n",
    "                        song_structure.descriptors[section].pos_std\n",
    "                        ])\n",
    "        all_rows.append(values)\n",
    "    except ValueError as e:\n",
    "        errors += 1\n",
    "        # print(song, e)\n",
    "\n",
    "columns = [\"song_id\"]\n",
    "for section in Section:\n",
    "    columns.extend([f\"amt_{section.value.lower()}\", f\"pos_avg_{section.value.lower()}\", f\"pos_std_{section.value.lower()}\"])\n",
    "\n",
    "print(errors, \"errors due to invalid brackets\")\n",
    "song_structure_df = pd.DataFrame(all_rows, columns=columns)\n",
    "song_structure_df.set_index(\"song_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbca319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.vq as scp\n",
    "\n",
    "observations = song_structure_df.reset_index()\n",
    "observations = observations.loc[:, ~observations.columns.isin([\"song_id\"])]\n",
    "normalized_observations = scp.whiten(obs=observations)\n",
    "assert normalized_observations.shape[1] == len(Section) * 3, f\"There should be {len(Section) * 3} features in the vector, as we have 3 features per section and {len(Section)} sections\"\n",
    "\n",
    "# assert True == False, \"Do not run this expensive computation below again... Takes 10 mins\"\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.kmeans.html\n",
    "distortions : list[tuple[int, np.float64]] = []\n",
    "for k in range(2, 100):\n",
    "    _, distortion = scp.kmeans(normalized_observations, k)\n",
    "    distortions.append((k, distortion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7154e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "\n",
    "ax1.set_title(\"Average distortion with k clusters\")\n",
    "ax1.set_ylabel(\"Distortion\")\n",
    "ax1.set_xlabel(\"k\")\n",
    "\n",
    "ax2.set_title(\"Distortion difference\")\n",
    "ax2.set_ylabel(\"$\\Delta$Distortion\")\n",
    "ax2.set_xlabel(\"k\")\n",
    "\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "\n",
    "sns.scatterplot(x=map(lambda x: x[0], distortions), y=map(lambda x: x[1], distortions), ax=ax1)\n",
    "distances = list(map(lambda t: t[1][1] - t[0][1], zip(distortions, distortions[1:])))\n",
    "sns.lineplot(distances, ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e46fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set k = 20 clusters, as we see a marginalized distortion difference from there\n",
    "k = 20\n",
    "codebook, _ =  scp.kmeans(normalized_observations, k)\n",
    "assigned_clusters, obs_distortions = scp.vq(obs=normalized_observations, code_book=codebook)\n",
    "\n",
    "\n",
    "sns.kdeplot(obs_distortions)\n",
    "plt.title(f\"Density of distortions for {len(obs_distortions)} songs and {k} clusters\")\n",
    "plt.xlabel(\"Distortion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f6f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_structure_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the clusters to the dataframe and store them in the database\n",
    "\n",
    "song_structure_df['cluster'] = assigned_clusters\n",
    "song_cluster = [(int(song_id), int(cluster)) for song_id, cluster in song_structure_df[[\"cluster\"]].to_records(index=True)]\n",
    "\n",
    "conn, cursor = create_connection(\"genius_db\")\n",
    "cursor.executemany(\"INSERT INTO cluster (song_id, cluster_id) VALUES (%s,%s)\", song_cluster)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
